{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1ek3-yvKNDOokvqubI1-hzAqPSc7wYo4V",
      "authorship_tag": "ABX9TyPf4KSDecA/AMCnAFt2DQpe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linjay-Chian/Colab_sync_test/blob/main/GiA_Live_testing_analysis_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "collapsed": true,
        "id": "O5aHIJZhOs2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speech to Text Log"
      ],
      "metadata": {
        "id": "q5-MLmDYXVAD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe54722b"
      },
      "source": [
        "import os\n",
        "\n",
        "input_folder = \"STT Input Folder\"\n",
        "\n",
        "# Create the folder if it doesn't already exist\n",
        "if not os.path.exists(input_folder):\n",
        "    os.makedirs(input_folder)\n",
        "    print(f\"Folder '{input_folder}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Folder '{input_folder}' already exists.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb0441f8"
      },
      "source": [
        "import os\n",
        "\n",
        "output_folder = \"STT Output Folder\"\n",
        "\n",
        "# Create the folder if it doesn't already exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "    print(f\"Folder '{output_folder}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Folder '{output_folder}' already exists.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "Q_script_folder = \"Q_script Folder\"\n",
        "\n",
        "# Create the folder if it doesn't already exist\n",
        "if not os.path.exists(Q_script_folder):\n",
        "    os.makedirs(Q_script_folder)\n",
        "    print(f\"Folder '{Q_script_folder}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Folder '{Q_script_folder}' already exists.\")"
      ],
      "metadata": {
        "id": "4MYIyCSIhAmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b791c9c9"
      },
      "source": [
        "### Interactive File Upload with `google.colab.files.upload()`\n",
        "\n",
        "This method allows users to select and upload one or more files directly from their local machine to the Colab environment. The uploaded files are temporarily stored in the Colab VM's filesystem.\n",
        "\n",
        "After execution, a file selection dialog will appear. Choose your file(s) and click 'Open' or 'Upload'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9806b70"
      },
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# This will open a file selection dialog in your browser\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# uploaded_files is a dictionary where keys are filenames and values are file contents (bytes)\n",
        "print(f\"Successfully uploaded {len(uploaded_files)} file(s).\")\n",
        "\n",
        "for filename, content in uploaded_files.items():\n",
        "    print(f\"Reading content of '{filename}':\")\n",
        "    # You can now process the file content\n",
        "    # For text files, you might want to decode and read lines:\n",
        "    try:\n",
        "        # Assuming it's a text file for demonstration\n",
        "        file_content_str = content.decode('utf-8')\n",
        "        print(f\"First 100 characters of {filename}:\\n{file_content_str[:100]}...\")\n",
        "\n",
        "        # If you need to save it to the Colab filesystem:\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(content)\n",
        "        print(f\"'{filename}' saved to Colab local storage.\")\n",
        "\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"'{filename}' might not be a text file or has a different encoding.\")\n",
        "        print(f\"Size of {filename}: {len(content)} bytes.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "893787f1"
      },
      "source": [
        "### Interactive File Selection from Colab Virtual Disk using `ipywidgets`\n",
        "\n",
        "This method allows you to display files already present on the Colab VM's filesystem and let the user select one from a dropdown list. This is useful when you have files you've generated or previously uploaded to the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2675cf0"
      },
      "source": [
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "def get_files_in_directory(directory='.', file_extension=None):\n",
        "    \"\"\"\n",
        "    Lists files in a given directory, optionally filtered by extension.\n",
        "    \"\"\"\n",
        "    files = []\n",
        "    for f in os.listdir(directory):\n",
        "        full_path = os.path.join(directory, f)\n",
        "        if os.path.isfile(full_path):\n",
        "            if file_extension is None or f.endswith(file_extension):\n",
        "                files.append(full_path)\n",
        "    return sorted(files)\n",
        "\n",
        "# --- Configuration for file selection ---\n",
        "search_directory = 'STT Output Folder'  # Changed to point to the folder with generated CSVs\n",
        "file_extension = '.csv' # Filter for specific file types, or set to None for all files\n",
        "\n",
        "available_files = get_files_in_directory(search_directory, file_extension)\n",
        "\n",
        "if not available_files:\n",
        "    print(f\"No {file_extension if file_extension else 'any'} files found in {search_directory}.\")\n",
        "else:\n",
        "    print(f\"Found {len(available_files)} {file_extension if file_extension else 'any'} files in {search_directory}.\")\n",
        "\n",
        "    # Create a dropdown widget\n",
        "    file_dropdown = widgets.Dropdown(\n",
        "        options=available_files,\n",
        "        description='Select File:',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Create an output widget to display messages\n",
        "    output_widget = widgets.Output()\n",
        "\n",
        "    # Variable to store the selected file path (this is in the global scope)\n",
        "    selected_file_path = None\n",
        "\n",
        "    def on_button_click(b):\n",
        "        # Use 'global' to modify the module-level 'selected_file_path'\n",
        "        global selected_file_path\n",
        "        selected_file_path = file_dropdown.value\n",
        "        with output_widget:\n",
        "            clear_output()\n",
        "            print(f\"Selected file: {selected_file_path}\")\n",
        "\n",
        "    # Create a button to confirm selection\n",
        "    select_button = widgets.Button(description=\"Confirm Selection\")\n",
        "    select_button.on_click(on_button_click)\n",
        "\n",
        "    # Display the widgets\n",
        "    display(file_dropdown, select_button, output_widget)\n",
        "\n",
        "    # You can now use `selected_file_path` in subsequent cells after the user clicks the button\n",
        "    # For example, to read the selected CSV file:\n",
        "    # if selected_file_path:\n",
        "    #     df = pd.read_csv(selected_file_path)\n",
        "    #     print(df.head())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5c3a4c3"
      },
      "source": [
        "After running the above cell, a dropdown and a 'Confirm Selection' button will appear. Make your choice and click the button. The selected file path will be stored in the `selected_file_path` variable and printed in the output area.\n",
        "\n",
        "You can then use this `selected_file_path` variable in subsequent code cells to load or process the chosen file, for example:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "if 'selected_file_path' in globals() and selected_file_path:\n",
        "    try:\n",
        "        df = pd.read_csv(selected_file_path)\n",
        "        print(f\"Successfully loaded '{selected_file_path}' into a DataFrame.\")\n",
        "        display(df.head())\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {selected_file_path}: {e}\")\n",
        "else:\n",
        "    print(\"No file has been selected yet. Please select a file and click 'Confirm Selection' above.\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "import tkinter as tk\n",
        "from tkinter import filedialog, messagebox\n",
        "import os  # Added for directory and path operations\n",
        "\n",
        "def parse_log_data(input_file_path):\n",
        "    \"\"\"\n",
        "    Parses a log file and returns the extracted data rows.\n",
        "\n",
        "    The function reads the input file, processes it line by line to find\n",
        "    matching data patterns, and collects the data into a list. It also\n",
        "    handles potential errors during file processing.\n",
        "\n",
        "    Args:\n",
        "        input_file_path (str): The path to the input text file.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of lists, where each inner list is a row of data.\n",
        "              Returns an empty list if an error occurs or no data is found.\n",
        "    \"\"\"\n",
        "    data_rows = []\n",
        "    try:\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        i = 0\n",
        "        while i < len(lines) - 1:\n",
        "            line1 = lines[i].strip()\n",
        "            line2 = lines[i+1].strip()\n",
        "\n",
        "            # Use regex to find the date, time, and number in the first line.\n",
        "            match = re.match(r'(\\d{2}/\\d{2})\\s+(\\d{2}:\\d{2}:\\d{2})\\s+(\\d+)', line1)\n",
        "\n",
        "            # Check if the first line matches and the second line has the content.\n",
        "            if match and line2.startswith('multishot-voice:'):\n",
        "                date = match.group(1)\n",
        "                time = match.group(2)\n",
        "                number = match.group(3)\n",
        "\n",
        "                # Extract content by splitting the string.\n",
        "                content = line2.split('multishot-voice:', 1)[1].strip()\n",
        "\n",
        "                data_rows.append([date, time, number, content])\n",
        "\n",
        "                # The data pattern spans 3 lines (data, content, 's3'), so we skip ahead.\n",
        "                i += 3\n",
        "            else:\n",
        "                # If the lines don't match our pattern, move to the next line.\n",
        "                i += 1\n",
        "        return data_rows\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        # For batch processing, we'll print errors instead of showing a popup\n",
        "        print(f\"Error: The file '{input_file_path}' was not found.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Parsing Error in {input_file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "def write_data_to_csv(output_file_path, headers, data_rows, show_success_popup=True):\n",
        "    \"\"\"\n",
        "    Writes the given headers and data rows to a specified CSV file.\n",
        "\n",
        "    Args:\n",
        "        output_file_path (str): The path to save the output CSV file.\n",
        "        headers (list): A list of strings for the CSV header row.\n",
        "        data_rows (list): A list of lists containing the data to write.\n",
        "        show_success_popup (bool): If True, shows a success message box. (Default is True)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(output_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow(headers)\n",
        "            writer.writerows(data_rows)\n",
        "\n",
        "        if show_success_popup:\n",
        "            print(f\"Success: Successfully saved data to {output_file_path}\\nTotal rows written: {len(data_rows)}\")\n",
        "    except Exception as e:\n",
        "        # Show error for saving, as this might be a permissions issue\n",
        "        print(f\"Saving Error: An unexpected error occurred while writing file {output_file_path}:\\n{e}\")\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Set up the root Tkinter window and hide it, as we only need the dialogs.\n",
        "    #root = tk.Tk()\n",
        "    #root.withdraw()\n",
        "\n",
        "    # --- MODIFIED: Ask for an input DIRECTORY ---\n",
        "    input_folder = 'STT Input Folder'\n",
        "    output_folder = 'STT Output Folder'\n",
        "\n",
        "    # If the user selected a folder (didn't cancel the dialog)\n",
        "    if input_folder:\n",
        "        headers = ['Date', 'Time', 'Number', 'Content']\n",
        "        files_processed = 0\n",
        "        files_failed = 0\n",
        "\n",
        "        # --- NEW: Iterate through all files in the selected folder ---\n",
        "        for filename in os.listdir(input_folder):\n",
        "\n",
        "            # Process only .txt files\n",
        "            if filename.endswith(\".txt\"):\n",
        "                input_file_path = os.path.join(input_folder, filename)\n",
        "\n",
        "                # --- NEW: Auto-generate output file path ---\n",
        "                # Get the filename without the .txt extension\n",
        "                base_filename = os.path.splitext(filename)[0]\n",
        "                # Create the new .csv filename\n",
        "                output_filename = f\"{base_filename}.csv\"\n",
        "                # Create the full path to save the new file\n",
        "                output_file_path = os.path.join(output_folder, output_filename)\n",
        "\n",
        "                try:\n",
        "                    # Process the selected file to extract data.\n",
        "                    processed_data = parse_log_data(input_file_path)\n",
        "\n",
        "                    # If parsing was successful and returned data...\n",
        "                    if processed_data:\n",
        "                        # Sort the data by the 'Time' column (the second element in each row)\n",
        "                        processed_data.sort(key=lambda row: row[1])\n",
        "\n",
        "                        # Write the processed data to the new CSV file\n",
        "                        # Pass show_success_popup=False to prevent a popup for every file\n",
        "                        write_data_to_csv(output_file_path, headers, processed_data, show_success_popup=False)\n",
        "                        files_processed += 1\n",
        "                    # else: No data found in the file, just skip it.\n",
        "\n",
        "                except Exception as e:\n",
        "                    # Log errors to console\n",
        "                    print(f\"Error processing file {input_file_path}: {e}\")\n",
        "                    files_failed += 1\n",
        "\n",
        "        # --- NEW: Show a final summary message ---\n",
        "        if files_processed > 0 or files_failed > 0:\n",
        "            print(f\"Processing Complete:\\nSuccessfully processed: {files_processed} files.\\nFailed to process: {files_failed} files.\\n\\nCSV files saved in:\\n{output_folder}\")\n",
        "        else:\n",
        "            print(\"No Files Found: No .txt files were found or processed in the selected folder.\")\n",
        "\n",
        "    # else: User cancelled the folder selection, the script will just exit.\n"
      ],
      "metadata": {
        "id": "Milvsl_1bPh0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Response Log"
      ],
      "metadata": {
        "id": "I8g6XFyJXtvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scores Calculation"
      ],
      "metadata": {
        "id": "Niu2oUTXNDVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WER Calculation"
      ],
      "metadata": {
        "id": "vJWTSMagNNax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, ensure you have the necessary libraries installed:\n",
        "# pip install pandas \"jiwer>=2.0.0\"\n",
        "\n",
        "import pandas as pd\n",
        "import jiwer\n",
        "import string # Import the string module to access punctuation\n",
        "import os # Import the os module to check for file paths\n",
        "\n",
        "def calculate_overall_wer(dataframe: pd.DataFrame, ground_truth_col: str, hypothesis_col: str) -> object:\n",
        "    \"\"\"\n",
        "    Calculates the overall Word Error Rate (WER) across the entire DataFrame,\n",
        "    ignoring case and punctuation.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pd.DataFrame): The DataFrame containing the text data.\n",
        "        ground_truth_col (str): The name of the column with the correct reference transcripts.\n",
        "        hypothesis_col (str): The name of the column with the ASR output.\n",
        "\n",
        "    Returns:\n",
        "        object: An object from jiwer.process_words containing the overall WER and error counts.\n",
        "    \"\"\"\n",
        "    # Ensure the columns exist in the DataFrame\n",
        "    if ground_truth_col not in dataframe.columns or hypothesis_col not in dataframe.columns:\n",
        "        raise ValueError(f\"One or both specified columns ('{ground_truth_col}', '{hypothesis_col}') are not in the DataFrame.\")\n",
        "\n",
        "    # Create a translation table to remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "    # Clean the text: remove punctuation, convert to lowercase, handle missing values\n",
        "    ground_truth = dataframe[ground_truth_col].fillna('').astype(str).str.translate(translator).str.lower().tolist()\n",
        "    hypothesis = dataframe[hypothesis_col].fillna('').astype(str).str.translate(translator).str.lower().tolist()\n",
        "\n",
        "    # Process the entire corpus to get overall metrics\n",
        "    error_measures = jiwer.process_words(ground_truth, hypothesis)\n",
        "\n",
        "    return error_measures\n",
        "\n",
        "def get_row_error_breakdown(row: pd.Series, ground_truth_col: str, hypothesis_col: str) -> str:\n",
        "    \"\"\"\n",
        "    Calculates the error breakdown for a single row of the DataFrame,\n",
        "    ignoring case and punctuation.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): A single row of the DataFrame.\n",
        "        ground_truth_col (str): The name of the ground truth column.\n",
        "        hypothesis_col (str): The name of the hypothesis column.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string showing the substitutions (S), deletions (D),\n",
        "             and insertions (I) for the row.\n",
        "    \"\"\"\n",
        "    # Create a translation table to remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "    # Clean the text: remove punctuation and convert to lowercase\n",
        "    ground_truth = str(row[ground_truth_col]).translate(translator).lower()\n",
        "    hypothesis = str(row[hypothesis_col]).translate(translator).lower()\n",
        "\n",
        "    # Process just the single pair of sentences\n",
        "    measures = jiwer.process_words(ground_truth, hypothesis)\n",
        "\n",
        "    # Format the breakdown string\n",
        "    breakdown = f\"S: {measures.substitutions}, D: {measures.deletions}, I: {measures.insertions}\"\n",
        "    return breakdown\n",
        "\n",
        "\n",
        "# --- Main execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Set fixed file paths ---\n",
        "    # The input file must be in the same directory as the script, or provide a full path.\n",
        "    input_filename = 'FinalDataFrame.csv'\n",
        "    output_csv_filename = 'output_1.csv'\n",
        "\n",
        "    try:\n",
        "        # --- Load the specified CSV file ---\n",
        "        print(f\"Loading input file: '{input_filename}'...\")\n",
        "        df = pd.read_csv(input_filename)\n",
        "        print(\"File loaded successfully.\")\n",
        "\n",
        "        # Define the column names for clarity\n",
        "        ground_truth_column = 'Scripts'\n",
        "        hypothesis_column = 'Sherlog_STT'\n",
        "\n",
        "        # --- 1. Calculate and print overall WER ---\n",
        "        overall_wer_results = calculate_overall_wer(df, ground_truth_column, hypothesis_column)\n",
        "\n",
        "        # --- 2. Prepare the summary report for console and file ---\n",
        "        summary_lines = []\n",
        "        summary_lines.append(\"--- Overall Word Error Rate (WER) Calculation Results (Case-Insensitive, No Punctuation) ---\")\n",
        "        summary_lines.append(f\"Comparing hypothesis ('{hypothesis_column}') to ground truth ('{ground_truth_column}')\\n\")\n",
        "        summary_lines.append(f\"Overall Word Error Rate (WER): {overall_wer_results.wer:.2%}\")\n",
        "        summary_lines.append(f\"Overall Merge Error Rate (MER): {overall_wer_results.mer:.2%}\")\n",
        "        summary_lines.append(f\"Overall Word Information Lost (WIL): {overall_wer_results.wil:.2f}\\n\")\n",
        "        summary_lines.append(\"--- Overall Detailed Error Breakdown ---\")\n",
        "        summary_lines.append(f\"Hits (Correct Words):         {overall_wer_results.hits}\")\n",
        "        summary_lines.append(f\"Substitutions (S):            {overall_wer_results.substitutions}\")\n",
        "        summary_lines.append(f\"Deletions (D):                {overall_wer_results.deletions}\")\n",
        "        summary_lines.append(f\"Insertions (I):               {overall_wer_results.insertions}\\n\")\n",
        "\n",
        "        total_words_truth = overall_wer_results.hits + overall_wer_results.substitutions + overall_wer_results.deletions\n",
        "        total_words_hypothesis = overall_wer_results.hits + overall_wer_results.substitutions + overall_wer_results.insertions\n",
        "\n",
        "        summary_lines.append(\"--- Overall Corpus Statistics ---\")\n",
        "        summary_lines.append(f\"Total Words in Ground Truth:  {total_words_truth}\")\n",
        "        summary_lines.append(f\"Total Words in Hypothesis:    {total_words_hypothesis}\")\n",
        "\n",
        "        # Print the summary to the console\n",
        "        print(\"\\n\".join(summary_lines))\n",
        "\n",
        "        # --- 3. Calculate row-by-row breakdown and add to DataFrame ---\n",
        "        print(\"\\n--- Processing row-by-row error breakdown ---\")\n",
        "        df['Error_Breakdown'] = df.apply(\n",
        "            get_row_error_breakdown,\n",
        "            axis=1, # Apply function to each row\n",
        "            args=(ground_truth_column, hypothesis_column)\n",
        "        )\n",
        "        print(\"Successfully added 'Error_Breakdown' column.\")\n",
        "\n",
        "        # --- 4. Export the results to CSV and TXT files ---\n",
        "        # Export DataFrame to CSV\n",
        "        df.to_csv(output_csv_filename, index=False, encoding='utf-8')\n",
        "        print(f\"\\nResults successfully exported to '{output_csv_filename}'\")\n",
        "\n",
        "        # Generate the text file name from the CSV file name\n",
        "        base_name = os.path.splitext(output_csv_filename)[0]\n",
        "        output_txt_filename = base_name + \"_summary.txt\"\n",
        "\n",
        "        # Write the summary report to a text file\n",
        "        with open(output_txt_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"\\n\".join(summary_lines))\n",
        "        print(f\"Overall summary successfully exported to '{output_txt_filename}'\")\n",
        "\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    except FileNotFoundError:\n",
        "         print(f\"Error: The file '{input_filename}' was not found. Please make sure it's in the same directory as the script.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "yglxVjXdNRkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ComprehensiveScores"
      ],
      "metadata": {
        "id": "V7FrOlm8Perg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import re\n",
        "\n",
        "def parse_gemini_bulk_response(response_text):\n",
        "    \"\"\"\n",
        "    Parses a multi-row markdown table from the Gemini response to extract evaluation data.\n",
        "    Returns a list of dictionaries, where each dictionary represents a row.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    lines = response_text.strip().splitlines()\n",
        "\n",
        "    # Find all lines that are part of the markdown table\n",
        "    table_lines = [line for line in lines if line.strip().startswith('|')]\n",
        "\n",
        "    # The first table line is the header, the second is the separator. Data starts from the third.\n",
        "    # This robustly skips the header and separator, preventing the header from being treated as data.\n",
        "    if len(table_lines) < 3:\n",
        "        # Not enough lines for a valid table with at least one data row\n",
        "        return []\n",
        "\n",
        "    data_lines = table_lines[2:] # Skip the | Header | and |---| lines\n",
        "\n",
        "    for line in data_lines:\n",
        "        # Use regex to robustly capture content within pipes.\n",
        "        # This handles cases where columns might be empty.\n",
        "        match = re.search(r'\\|\\s*(.*?)\\s*\\|\\s*(.*?)\\s*\\|\\s*(.*?)\\s*\\|\\s*(.*?)\\s*\\|', line)\n",
        "        if match:\n",
        "            question, answer, score, justification = [m.strip() for m in match.groups()]\n",
        "            results.append({\n",
        "                \"Question\": question,\n",
        "                \"Answer\": answer,\n",
        "                \"Score (1-10)\": score,\n",
        "                \"Justification\": justification\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def evaluate_qa_pairs(input_csv_path):\n",
        "    \"\"\"\n",
        "    Reads all question-answer pairs from a CSV file, sends them to the Gemini API in a single batch,\n",
        "    and returns the structured evaluation as a list of dictionaries.\n",
        "    \"\"\"\n",
        "    # --- 1. API Configuration ---\n",
        "    # It's recommended to use environment variables for API keys for better security.\n",
        "    # For example: api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
        "    api_key = \"AIzaSyDorS4mD54chTzau2WahFrHYcATpNv5lhg\" # Please replace with your actual API key or use environment variables\n",
        "    if api_key == \"YOUR_API_KEY_HERE\":\n",
        "        print(\"Warning: Please replace 'YOUR_API_KEY_HERE' with your actual Gemini API key.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        genai.configure(api_key=api_key)\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring the Gemini API. Please check your API key. Details: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 2. Model Initialization ---\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-2.5-pro') # Updated to a recommended model\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing the model. Please check the model name and your API access. Details: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 3. Load the CSV File ---\n",
        "    try:\n",
        "        df = pd.read_csv(input_csv_path)\n",
        "        print(f\"Successfully loaded '{os.path.basename(input_csv_path)}'.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{input_csv_path}' was not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the CSV file. Details: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 4. Validate Columns ---\n",
        "    if \"Scripts\" not in df.columns or \"Response\" not in df.columns:\n",
        "        print(\"Error: The CSV file must contain a 'Scripts' column for questions and a 'Response' column for answers.\")\n",
        "        return None\n",
        "\n",
        "    # --- 5. Prepare all Q&A pairs for the prompt ---\n",
        "    qa_pairs_string = \"\"\n",
        "    original_qa = []\n",
        "    for index, row in df.iterrows():\n",
        "        question = str(row['Scripts']).strip()\n",
        "        answer = str(row['Response']).strip()\n",
        "\n",
        "        if question and answer and question.lower() != 'nan' and answer.lower() != 'nan':\n",
        "            qa_pairs_string += f\"Question: \\\"{question}\\\"\\nAnswer: \\\"{answer}\\\"\\n---\\n\"\n",
        "            original_qa.append({\"Question\": question, \"Answer\": answer})\n",
        "\n",
        "    if not qa_pairs_string:\n",
        "        print(\"No valid Q&A pairs found in the file.\")\n",
        "        return []\n",
        "\n",
        "    # --- 6. Construct a Single Prompt for all pairs ---\n",
        "    prompt = f\"\"\"\n",
        "    Please act as an impartial evaluator. Your task is to analyze all of the following question-and-answer pairs.\n",
        "    Provide your complete evaluation in a single, multi-row markdown table. Each row in the table must correspond to one Q&A pair.\n",
        "\n",
        "    **Q&A Pairs to Evaluate:**\n",
        "    {qa_pairs_string}\n",
        "\n",
        "    **Output Format (Strict):**\n",
        "    Create a single markdown table with exactly four columns: \"Question\", \"Answer\", \"Score (1-10)\", and \"Justification\".\n",
        "    The table must have one data row for each Q&A pair provided above. Do not include any text outside of this table.\n",
        "    | Question | Answer | Score (1-10) | Justification |\n",
        "    |---|---|---|---|\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Sending all Q&A pairs to Gemini for evaluation in a single batch...\")\n",
        "\n",
        "    try:\n",
        "        # --- 7. Generate Content with a Single API Call ---\n",
        "        response = model.generate_content(prompt)\n",
        "        print(\"Received response from Gemini. Parsing results...\")\n",
        "\n",
        "        parsed_data = parse_gemini_bulk_response(response.text)\n",
        "\n",
        "        if parsed_data:\n",
        "            print(f\"Successfully parsed {len(parsed_data)} entries from the response.\")\n",
        "            return parsed_data\n",
        "        else:\n",
        "            print(\"Could not parse the response from Gemini. The raw response was:\")\n",
        "            print(response.text)\n",
        "            # Return error placeholder for user to inspect the output file\n",
        "            return [{\"Question\": \"PARSE_ERROR\", \"Answer\": \"Could not parse Gemini's response.\", \"Score (1-10)\": \"N/A\", \"Justification\": response.text}]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"--- An error occurred with the API call: {e} ---\")\n",
        "        return [{\"Question\": \"API_ERROR\", \"Answer\": str(e), \"Score (1-10)\": \"N/A\", \"Justification\": \"The API call failed.\"}]\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- 1. Set fixed input and output file names ---\n",
        "    input_file = 'FinalDataFrame.csv'\n",
        "    output_file = 'output_2.csv'\n",
        "\n",
        "    print(f\"Input file set to: {input_file}\")\n",
        "    print(f\"Output file will be saved as: {output_file}\")\n",
        "\n",
        "    # --- 2. Run the evaluation ---\n",
        "    results = evaluate_qa_pairs(input_file)\n",
        "\n",
        "    # --- 3. Save results to new CSV ---\n",
        "    if results:\n",
        "        print(f\"\\nEvaluation complete. Processed {len(results)} entries.\")\n",
        "        output_df = pd.DataFrame(results)\n",
        "\n",
        "        try:\n",
        "            output_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
        "            print(f\"Results successfully saved to '{output_file}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save the results to CSV. Error: {e}\")\n",
        "    else:\n",
        "        print(\"Evaluation did not produce any results.\")\n"
      ],
      "metadata": {
        "id": "ZOu8mbKqP3En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ContentSwapScores"
      ],
      "metadata": {
        "id": "HQCYuPcePGN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "# import tkinter as tk  # No longer needed for file dialogs\n",
        "# from tkinter import filedialog, messagebox # No longer needed for file dialogs\n",
        "\n",
        "# --- Configuration ---\n",
        "LOW_IMPACT_SWAPS = {\n",
        "    frozenset({'at', 'in'}),\n",
        "    frozenset({'on', 'in'}),\n",
        "    frozenset({'a', 'the'}),\n",
        "    frozenset({'a', 'of'}),\n",
        "}\n",
        "NUMBER_WORDS = {'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', 'ten': '10'}\n",
        "\n",
        "# NEW: Add a list of common, low-impact \"stop words\".\n",
        "# Deleting one of these is a minor error. Deleting anything else is critical.\n",
        "STOP_WORDS = {\n",
        "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'in', 'on', 'at', 'of',\n",
        "    'to', 'for', 'by', 'with', 'and', 'or', 'but', 'if', 'be'\n",
        "}\n",
        "\n",
        "# --- Normalization and Reconciliation Functions ---\n",
        "def normalize_text(text: str) -> str:\n",
        "    # ... (This function is unchanged from your provided version) ...\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = text.lower()\n",
        "    text = text.replace('$', 'dollar ')\n",
        "    text = re.sub(r'(\\w+)-dollar', r'dollar \\1', text)\n",
        "    text = re.sub(r'\\b(are)\\b', \"'re\", text)\n",
        "    text = re.sub(r\"[’‘´]\", \"'\", text)\n",
        "    text = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', text)\n",
        "    text = ' '.join([NUMBER_WORDS.get(word, word) for word in text.split()])\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    text = text.strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "def reconcile_spacing_errors(words1: list, words2: list) -> bool:\n",
        "    # ... (This function is unchanged) ...\n",
        "    if len(words1) > len(words2): longer_list, shorter_list = words1, words2\n",
        "    elif len(words2) > len(words1): longer_list, shorter_list = words2, words1\n",
        "    else: return False\n",
        "    shorter_word_set = set(shorter_list)\n",
        "    for i in range(len(longer_list) - 1):\n",
        "        combined_word = longer_list[i] + longer_list[i+1]\n",
        "        if combined_word in shorter_word_set:\n",
        "            repaired_list = longer_list[:i] + [combined_word] + longer_list[i+2:]\n",
        "            if sorted(repaired_list) == sorted(shorter_list):\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# --- Scoring Functions ---\n",
        "# ... (These functions are unchanged) ...\n",
        "def get_substitution_cost(c1: str, c2: str) -> float:\n",
        "    vowels = \"aeiou\"\n",
        "    phonetically_similar_pairs = [('t', 'd'), ('p', 'b'), ('k', 'g'), ('s', 'z'), ('f', 'v')]\n",
        "    if c1 == c2: return 0.0\n",
        "    if c1 in vowels and c2 in vowels: return 0.4\n",
        "    for p1, p2 in phonetically_similar_pairs:\n",
        "        if (c1 == p1 and c2 == p2) or (c1 == p2 and c2 == p1): return 0.5\n",
        "    return 1.0\n",
        "\n",
        "def weighted_levenshtein(s1: str, s2: str) -> float:\n",
        "    if len(s1) < len(s2): return weighted_levenshtein(s2, s1)\n",
        "    if len(s2) == 0: return float(len(s1))\n",
        "    previous_row = list(range(len(s2) + 1))\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions, deletions = previous_row[j + 1] + 1, current_row[j] + 1\n",
        "            substitutions = previous_row[j] + get_substitution_cost(c1, c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    return float(previous_row[-1])\n",
        "\n",
        "def calculate_jaccard_similarity(set1: set, set2: set) -> float:\n",
        "    if not set1 and not set2: return 1.0\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union != 0 else 0.0\n",
        "\n",
        "# --- MODIFIED: Intent Classification Function ---\n",
        "def classify_intent(norm_source: str, norm_asr: str, semantic_score: float) -> str:\n",
        "    source_words, asr_words = set(norm_source.split()), set(norm_asr.split())\n",
        "    question_words = {'what', 'why', 'who', 'how', 'which', 'where', 'when'}\n",
        "    deleted_words, added_words = source_words - asr_words, asr_words - source_words\n",
        "\n",
        "    # --- Swap Rules (Highest Priority) ---\n",
        "    if len(deleted_words) == 1 and len(added_words) == 1:\n",
        "        swapped_pair = frozenset(deleted_words.union(added_words))\n",
        "        if swapped_pair not in LOW_IMPACT_SWAPS:\n",
        "            return \"Swaps Intent\"\n",
        "    source_qword, asr_qword = question_words.intersection(source_words), question_words.intersection(asr_words)\n",
        "    if source_qword and asr_qword and (source_qword != asr_qword):\n",
        "        return \"Swaps Intent\"\n",
        "\n",
        "    # --- NEW: Critical Deletion Rule ---\n",
        "    # If exactly one important (non-stop word) was deleted, the intent is obscured.\n",
        "    if len(deleted_words) == 1 and len(added_words) == 0:\n",
        "        deleted_word = list(deleted_words)[0]\n",
        "        if deleted_word not in STOP_WORDS:\n",
        "            return \"Obscures Intent\"\n",
        "    # --- END NEW RULE ---\n",
        "\n",
        "    # --- Other Obscures Intent Rules ---\n",
        "    if len(source_words) <= 5 and semantic_score != 1.0:\n",
        "        return \"Obscures Intent\"\n",
        "    if source_qword and not asr_qword:\n",
        "        return \"Obscures Intent\"\n",
        "    if semantic_score < 0.7:\n",
        "        return \"Obscures Intent\"\n",
        "\n",
        "    return \"Preserves Intent\"\n",
        "\n",
        "def get_risk_score(intent_category: str) -> int:\n",
        "    return {\"Preserves Intent\": 0, \"Obscures Intent\": 1, \"Swaps Intent\": 5}.get(intent_category, -1)\n",
        "\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    # MODIFIED: Removed the file dialog for a fixed input file name\n",
        "    # 1. Fixed the input file name\n",
        "    input_csv_path = 'FinalDataFrame.csv'\n",
        "\n",
        "    source_col = 'Source (Speaker1)'\n",
        "    asr_col = 'ASR on Device under test'\n",
        "    try:\n",
        "        df = pd.read_csv(input_csv_path, sep=',')\n",
        "        results = []\n",
        "        print(f\"Starting analysis on '{input_csv_path}'...\")\n",
        "        for index, row in df.iterrows():\n",
        "            source_signal, asr_hypothesis = row.get(source_col, \"\"), row.get(asr_col, \"\")\n",
        "            norm_source, norm_asr = normalize_text(source_signal), normalize_text(asr_hypothesis)\n",
        "            phonetic_score, semantic_score, intent_category, risk_score = 0.0, 1.0, \"Preserves Intent\", 0\n",
        "            if norm_source != norm_asr:\n",
        "                was_reconciled = reconcile_spacing_errors(norm_source.split(), norm_asr.split())\n",
        "                if was_reconciled:\n",
        "                    phonetic_score, semantic_score, intent_category, risk_score = 0.5, 1.0, \"Preserves Intent\", 0\n",
        "                else:\n",
        "                    phonetic_score = weighted_levenshtein(norm_source, norm_asr)\n",
        "                    source_set, asr_set = set(norm_source.split()), set(asr_hypothesis.split())\n",
        "                    semantic_score = calculate_jaccard_similarity(source_set, asr_set)\n",
        "                    intent_category = classify_intent(norm_source, norm_asr, semantic_score)\n",
        "                    risk_score = get_risk_score(intent_category)\n",
        "            results.append([phonetic_score, semantic_score, intent_category, risk_score])\n",
        "\n",
        "        new_columns_df = pd.DataFrame(results, columns=['Phonetic_Distance_Score', 'Semantic_Similarity_Score', 'Impact_on_Intent_Category', 'Intent_Impact_Risk_Score'])\n",
        "        cols_to_drop = new_columns_df.columns.tolist()\n",
        "        df_clean = df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors='ignore')\n",
        "        final_df = pd.concat([df_clean.reset_index(drop=True), new_columns_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "        # MODIFIED: Removed file dialog for a fixed output file name\n",
        "        # 2. Fixed the output file name\n",
        "        output_csv_path = 'output_3.csv'\n",
        "\n",
        "        final_df.to_csv(output_csv_path, index=False, sep=',')\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"✅ Analysis complete!\")\n",
        "        print(f\"Results have been saved to '{output_csv_path}'\")\n",
        "        print(\"-\" * 50)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ ERROR: Input file not found.\")\n",
        "        print(f\"Please make sure '{input_csv_path}' is in the same directory as the script.\")\n",
        "    except KeyError as e:\n",
        "        print(f\"❌ ERROR: A required column was not found in the CSV. Details: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "cDshZrTpPizl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conbine All CSV"
      ],
      "metadata": {
        "id": "xPFy05e1SDWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def combine_and_deduplicate_csvs(output_file):\n",
        "    \"\"\"\n",
        "    Combines the specific CSV files 'output_1.csv', 'output_2.csv', 'output_3.csv',\n",
        "    removes duplicate columns, and saves the result to a new CSV file.\n",
        "\n",
        "    Args:\n",
        "        output_file (str): The path for the output CSV file.\n",
        "    \"\"\"\n",
        "    # List of the specific input CSV files to combine\n",
        "    file_list = ['output_1.csv', 'output_2.csv', 'output_3.csv']\n",
        "\n",
        "    # Check if input files exist\n",
        "    for f in file_list:\n",
        "        if not os.path.exists(f):\n",
        "            print(f\"Error: Input file not found at '{f}'\")\n",
        "            return\n",
        "\n",
        "    try:\n",
        "        # Read each CSV file into a pandas DataFrame and store them in a list\n",
        "        df_list = [pd.read_csv(file) for file in file_list]\n",
        "\n",
        "        # Concatenate all DataFrames in the list into a single DataFrame horizontally\n",
        "        combined_df = pd.concat(df_list, axis=1)\n",
        "        print(f\"Total columns before deduplication: {len(combined_df.columns)}\")\n",
        "\n",
        "        # Remove duplicate columns by name, keeping the first occurrence\n",
        "        deduplicated_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n",
        "        print(f\"Total columns after deduplication: {len(deduplicated_df.columns)}\")\n",
        "\n",
        "        # Save the deduplicated DataFrame to a new CSV file\n",
        "        # index=False prevents pandas from writing row indices to the file\n",
        "        deduplicated_df.to_csv(output_file, index=False)\n",
        "        print(f\"Successfully combined and deduplicated files into '{output_file}'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # The script now uses a hardcoded list of files inside the function.\n",
        "    # We just need to define the output filename.\n",
        "    output_filename = 'combined_output.csv'\n",
        "\n",
        "    # Run the function\n",
        "    combine_and_deduplicate_csvs(output_filename)\n",
        "\n"
      ],
      "metadata": {
        "id": "FSZyWZxXSA1c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}