{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1ek3-yvKNDOokvqubI1-hzAqPSc7wYo4V",
      "authorship_tag": "ABX9TyPf4KSDecA/AMCnAFt2DQpe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e5636140bae141a58158bcd49325972f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "STT Output Folder/AG01_KCSON_-5SER.csv",
              "STT Output Folder/AG02_KCSON_-5SER.csv",
              "STT Output Folder/AH01_KCSON_-5SER.csv",
              "STT Output Folder/AH02_KCSON_-5SER.csv"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Select File:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_cf4f132891fe4906bf7e332cbd927712",
            "style": "IPY_MODEL_cd320d4cd6f64a1bb0ebfa92347a0d23"
          }
        },
        "cf4f132891fe4906bf7e332cbd927712": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd320d4cd6f64a1bb0ebfa92347a0d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e128d7b707c84f58accc467efa5665d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Confirm Selection",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_45d32d55a4ac4640b7eaeb687e92173e",
            "style": "IPY_MODEL_97781761155c44498cc5270dcf107d98",
            "tooltip": ""
          }
        },
        "45d32d55a4ac4640b7eaeb687e92173e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97781761155c44498cc5270dcf107d98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "325c03c33bc5405984e1071a7ca5bfac": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_3ee032c3bb394853a07c012b5fc86cca",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Selected file: STT Output Folder/AG01_KCSON_-5SER.csv\n"
                ]
              }
            ]
          }
        },
        "3ee032c3bb394853a07c012b5fc86cca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linjay-Chian/Colab_sync_test/blob/main/GiA_Live_testing_analysis_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O5aHIJZhOs2k",
        "outputId": "cc370d2e-bc75-4a5f-f6d3-9716e08e45a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-4.0.0 rapidfuzz-3.14.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speech to Text Log"
      ],
      "metadata": {
        "id": "q5-MLmDYXVAD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe54722b",
        "outputId": "5417204c-7b99-4135-aacb-12efa34be7a7"
      },
      "source": [
        "import os\n",
        "\n",
        "input_folder = \"STT Input Folder\"\n",
        "\n",
        "# Create the folder if it doesn't already exist\n",
        "if not os.path.exists(input_folder):\n",
        "    os.makedirs(input_folder)\n",
        "    print(f\"Folder '{input_folder}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Folder '{input_folder}' already exists.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder 'STT Input Folder' already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb0441f8",
        "outputId": "a964709d-f6ed-467d-cd41-8071afb2705f"
      },
      "source": [
        "import os\n",
        "\n",
        "output_folder = \"STT Output Folder\"\n",
        "\n",
        "# Create the folder if it doesn't already exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "    print(f\"Folder '{output_folder}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Folder '{output_folder}' already exists.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder 'STT Output Folder' already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "Q_script_folder = \"Q_script Folder\"\n",
        "\n",
        "# Create the folder if it doesn't already exist\n",
        "if not os.path.exists(Q_script_folder):\n",
        "    os.makedirs(Q_script_folder)\n",
        "    print(f\"Folder '{Q_script_folder}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Folder '{Q_script_folder}' already exists.\")"
      ],
      "metadata": {
        "id": "4MYIyCSIhAmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf103f4b-8a8a-4204-cf03-f999faae0de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder 'Q_script Folder' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b791c9c9"
      },
      "source": [
        "### Interactive File Upload with `google.colab.files.upload()`\n",
        "\n",
        "This method allows users to select and upload one or more files directly from their local machine to the Colab environment. The uploaded files are temporarily stored in the Colab VM's filesystem.\n",
        "\n",
        "After execution, a file selection dialog will appear. Choose your file(s) and click 'Open' or 'Upload'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "b9806b70",
        "outputId": "03596601-8bc1-4c84-fe9c-1539fbf8ad5a"
      },
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "# This will open a file selection dialog in your browser\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# uploaded_files is a dictionary where keys are filenames and values are file contents (bytes)\n",
        "print(f\"Successfully uploaded {len(uploaded_files)} file(s).\")\n",
        "\n",
        "for filename, content in uploaded_files.items():\n",
        "    print(f\"Reading content of '{filename}':\")\n",
        "    # You can now process the file content\n",
        "    # For text files, you might want to decode and read lines:\n",
        "    try:\n",
        "        # Assuming it's a text file for demonstration\n",
        "        file_content_str = content.decode('utf-8')\n",
        "        print(f\"First 100 characters of {filename}:\\n{file_content_str[:100]}...\")\n",
        "\n",
        "        # If you need to save it to the Colab filesystem:\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(content)\n",
        "        print(f\"'{filename}' saved to Colab local storage.\")\n",
        "\n",
        "    except UnicodeDecodeError:\n",
        "        print(f\"'{filename}' might not be a text file or has a different encoding.\")\n",
        "        print(f\"Size of {filename}: {len(content)} bytes.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7606a857-f7b8-41b3-9e30-5ba90e097819\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7606a857-f7b8-41b3-9e30-5ba90e097819\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3576004798.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# This will open a file selection dialog in your browser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# uploaded_files is a dictionary where keys are filenames and values are file contents (bytes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "893787f1"
      },
      "source": [
        "### Interactive File Selection from Colab Virtual Disk using `ipywidgets`\n",
        "\n",
        "This method allows you to display files already present on the Colab VM's filesystem and let the user select one from a dropdown list. This is useful when you have files you've generated or previously uploaded to the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "e5636140bae141a58158bcd49325972f",
            "cf4f132891fe4906bf7e332cbd927712",
            "cd320d4cd6f64a1bb0ebfa92347a0d23",
            "e128d7b707c84f58accc467efa5665d8",
            "45d32d55a4ac4640b7eaeb687e92173e",
            "97781761155c44498cc5270dcf107d98",
            "325c03c33bc5405984e1071a7ca5bfac",
            "3ee032c3bb394853a07c012b5fc86cca"
          ]
        },
        "id": "c2675cf0",
        "outputId": "b3e2ab11-c4ac-41c2-eb4c-25b415ce21c8"
      },
      "source": [
        "import os\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "def get_files_in_directory(directory='.', file_extension=None):\n",
        "    \"\"\"\n",
        "    Lists files in a given directory, optionally filtered by extension.\n",
        "    \"\"\"\n",
        "    files = []\n",
        "    for f in os.listdir(directory):\n",
        "        full_path = os.path.join(directory, f)\n",
        "        if os.path.isfile(full_path):\n",
        "            if file_extension is None or f.endswith(file_extension):\n",
        "                files.append(full_path)\n",
        "    return sorted(files)\n",
        "\n",
        "# --- Configuration for file selection ---\n",
        "search_directory = 'STT Output Folder'  # Changed to point to the folder with generated CSVs\n",
        "file_extension = '.csv' # Filter for specific file types, or set to None for all files\n",
        "\n",
        "available_files = get_files_in_directory(search_directory, file_extension)\n",
        "\n",
        "if not available_files:\n",
        "    print(f\"No {file_extension if file_extension else 'any'} files found in {search_directory}.\")\n",
        "else:\n",
        "    print(f\"Found {len(available_files)} {file_extension if file_extension else 'any'} files in {search_directory}.\")\n",
        "\n",
        "    # Create a dropdown widget\n",
        "    file_dropdown = widgets.Dropdown(\n",
        "        options=available_files,\n",
        "        description='Select File:',\n",
        "        disabled=False,\n",
        "    )\n",
        "\n",
        "    # Create an output widget to display messages\n",
        "    output_widget = widgets.Output()\n",
        "\n",
        "    # Variable to store the selected file path (this is in the global scope)\n",
        "    selected_file_path = None\n",
        "\n",
        "    def on_button_click(b):\n",
        "        # Use 'global' to modify the module-level 'selected_file_path'\n",
        "        global selected_file_path\n",
        "        selected_file_path = file_dropdown.value\n",
        "        with output_widget:\n",
        "            clear_output()\n",
        "            print(f\"Selected file: {selected_file_path}\")\n",
        "\n",
        "    # Create a button to confirm selection\n",
        "    select_button = widgets.Button(description=\"Confirm Selection\")\n",
        "    select_button.on_click(on_button_click)\n",
        "\n",
        "    # Display the widgets\n",
        "    display(file_dropdown, select_button, output_widget)\n",
        "\n",
        "    # You can now use `selected_file_path` in subsequent cells after the user clicks the button\n",
        "    # For example, to read the selected CSV file:\n",
        "    # if selected_file_path:\n",
        "    #     df = pd.read_csv(selected_file_path)\n",
        "    #     print(df.head())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4 .csv files in STT Output Folder.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Select File:', options=('STT Output Folder/AG01_KCSON_-5SER.csv', 'STT Output Folder/AG0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5636140bae141a58158bcd49325972f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Confirm Selection', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e128d7b707c84f58accc467efa5665d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "325c03c33bc5405984e1071a7ca5bfac"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5c3a4c3"
      },
      "source": [
        "After running the above cell, a dropdown and a 'Confirm Selection' button will appear. Make your choice and click the button. The selected file path will be stored in the `selected_file_path` variable and printed in the output area.\n",
        "\n",
        "You can then use this `selected_file_path` variable in subsequent code cells to load or process the chosen file, for example:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "if 'selected_file_path' in globals() and selected_file_path:\n",
        "    try:\n",
        "        df = pd.read_csv(selected_file_path)\n",
        "        print(f\"Successfully loaded '{selected_file_path}' into a DataFrame.\")\n",
        "        display(df.head())\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {selected_file_path}: {e}\")\n",
        "else:\n",
        "    print(\"No file has been selected yet. Please select a file and click 'Confirm Selection' above.\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "import tkinter as tk\n",
        "from tkinter import filedialog, messagebox\n",
        "import os  # Added for directory and path operations\n",
        "\n",
        "def parse_log_data(input_file_path):\n",
        "    \"\"\"\n",
        "    Parses a log file and returns the extracted data rows.\n",
        "\n",
        "    The function reads the input file, processes it line by line to find\n",
        "    matching data patterns, and collects the data into a list. It also\n",
        "    handles potential errors during file processing.\n",
        "\n",
        "    Args:\n",
        "        input_file_path (str): The path to the input text file.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of lists, where each inner list is a row of data.\n",
        "              Returns an empty list if an error occurs or no data is found.\n",
        "    \"\"\"\n",
        "    data_rows = []\n",
        "    try:\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        i = 0\n",
        "        while i < len(lines) - 1:\n",
        "            line1 = lines[i].strip()\n",
        "            line2 = lines[i+1].strip()\n",
        "\n",
        "            # Use regex to find the date, time, and number in the first line.\n",
        "            match = re.match(r'(\\d{2}/\\d{2})\\s+(\\d{2}:\\d{2}:\\d{2})\\s+(\\d+)', line1)\n",
        "\n",
        "            # Check if the first line matches and the second line has the content.\n",
        "            if match and line2.startswith('multishot-voice:'):\n",
        "                date = match.group(1)\n",
        "                time = match.group(2)\n",
        "                number = match.group(3)\n",
        "\n",
        "                # Extract content by splitting the string.\n",
        "                content = line2.split('multishot-voice:', 1)[1].strip()\n",
        "\n",
        "                data_rows.append([date, time, number, content])\n",
        "\n",
        "                # The data pattern spans 3 lines (data, content, 's3'), so we skip ahead.\n",
        "                i += 3\n",
        "            else:\n",
        "                # If the lines don't match our pattern, move to the next line.\n",
        "                i += 1\n",
        "        return data_rows\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        # For batch processing, we'll print errors instead of showing a popup\n",
        "        print(f\"Error: The file '{input_file_path}' was not found.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Parsing Error in {input_file_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "def write_data_to_csv(output_file_path, headers, data_rows, show_success_popup=True):\n",
        "    \"\"\"\n",
        "    Writes the given headers and data rows to a specified CSV file.\n",
        "\n",
        "    Args:\n",
        "        output_file_path (str): The path to save the output CSV file.\n",
        "        headers (list): A list of strings for the CSV header row.\n",
        "        data_rows (list): A list of lists containing the data to write.\n",
        "        show_success_popup (bool): If True, shows a success message box. (Default is True)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(output_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow(headers)\n",
        "            writer.writerows(data_rows)\n",
        "\n",
        "        if show_success_popup:\n",
        "            print(f\"Success: Successfully saved data to {output_file_path}\\nTotal rows written: {len(data_rows)}\")\n",
        "    except Exception as e:\n",
        "        # Show error for saving, as this might be a permissions issue\n",
        "        print(f\"Saving Error: An unexpected error occurred while writing file {output_file_path}:\\n{e}\")\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Set up the root Tkinter window and hide it, as we only need the dialogs.\n",
        "    #root = tk.Tk()\n",
        "    #root.withdraw()\n",
        "\n",
        "    # --- MODIFIED: Ask for an input DIRECTORY ---\n",
        "    input_folder = 'STT Input Folder'\n",
        "    output_folder = 'STT Output Folder'\n",
        "\n",
        "    # If the user selected a folder (didn't cancel the dialog)\n",
        "    if input_folder:\n",
        "        headers = ['Date', 'Time', 'Number', 'Content']\n",
        "        files_processed = 0\n",
        "        files_failed = 0\n",
        "\n",
        "        # --- NEW: Iterate through all files in the selected folder ---\n",
        "        for filename in os.listdir(input_folder):\n",
        "\n",
        "            # Process only .txt files\n",
        "            if filename.endswith(\".txt\"):\n",
        "                input_file_path = os.path.join(input_folder, filename)\n",
        "\n",
        "                # --- NEW: Auto-generate output file path ---\n",
        "                # Get the filename without the .txt extension\n",
        "                base_filename = os.path.splitext(filename)[0]\n",
        "                # Create the new .csv filename\n",
        "                output_filename = f\"{base_filename}.csv\"\n",
        "                # Create the full path to save the new file\n",
        "                output_file_path = os.path.join(output_folder, output_filename)\n",
        "\n",
        "                try:\n",
        "                    # Process the selected file to extract data.\n",
        "                    processed_data = parse_log_data(input_file_path)\n",
        "\n",
        "                    # If parsing was successful and returned data...\n",
        "                    if processed_data:\n",
        "                        # Sort the data by the 'Time' column (the second element in each row)\n",
        "                        processed_data.sort(key=lambda row: row[1])\n",
        "\n",
        "                        # Write the processed data to the new CSV file\n",
        "                        # Pass show_success_popup=False to prevent a popup for every file\n",
        "                        write_data_to_csv(output_file_path, headers, processed_data, show_success_popup=False)\n",
        "                        files_processed += 1\n",
        "                    # else: No data found in the file, just skip it.\n",
        "\n",
        "                except Exception as e:\n",
        "                    # Log errors to console\n",
        "                    print(f\"Error processing file {input_file_path}: {e}\")\n",
        "                    files_failed += 1\n",
        "\n",
        "        # --- NEW: Show a final summary message ---\n",
        "        if files_processed > 0 or files_failed > 0:\n",
        "            print(f\"Processing Complete:\\nSuccessfully processed: {files_processed} files.\\nFailed to process: {files_failed} files.\\n\\nCSV files saved in:\\n{output_folder}\")\n",
        "        else:\n",
        "            print(\"No Files Found: No .txt files were found or processed in the selected folder.\")\n",
        "\n",
        "    # else: User cancelled the folder selection, the script will just exit.\n"
      ],
      "metadata": {
        "id": "Milvsl_1bPh0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e86cd7-3da7-4496-e267-e458f8427a3d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Complete:\n",
            "Successfully processed: 4 files.\n",
            "Failed to process: 0 files.\n",
            "\n",
            "CSV files saved in:\n",
            "STT Output Folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Response Log"
      ],
      "metadata": {
        "id": "I8g6XFyJXtvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scores Calculation"
      ],
      "metadata": {
        "id": "Niu2oUTXNDVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WER Calculation"
      ],
      "metadata": {
        "id": "vJWTSMagNNax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, ensure you have the necessary libraries installed:\n",
        "# pip install pandas \"jiwer>=2.0.0\"\n",
        "\n",
        "import pandas as pd\n",
        "import jiwer\n",
        "import string # Import the string module to access punctuation\n",
        "import os # Import the os module to check for file paths\n",
        "\n",
        "def calculate_overall_wer(dataframe: pd.DataFrame, ground_truth_col: str, hypothesis_col: str) -> object:\n",
        "    \"\"\"\n",
        "    Calculates the overall Word Error Rate (WER) across the entire DataFrame,\n",
        "    ignoring case and punctuation.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pd.DataFrame): The DataFrame containing the text data.\n",
        "        ground_truth_col (str): The name of the column with the correct reference transcripts.\n",
        "        hypothesis_col (str): The name of the column with the ASR output.\n",
        "\n",
        "    Returns:\n",
        "        object: An object from jiwer.process_words containing the overall WER and error counts.\n",
        "    \"\"\"\n",
        "    # Ensure the columns exist in the DataFrame\n",
        "    if ground_truth_col not in dataframe.columns or hypothesis_col not in dataframe.columns:\n",
        "        raise ValueError(f\"One or both specified columns ('{ground_truth_col}', '{hypothesis_col}') are not in the DataFrame.\")\n",
        "\n",
        "    # Create a translation table to remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "    # Clean the text: remove punctuation, convert to lowercase, handle missing values\n",
        "    ground_truth = dataframe[ground_truth_col].fillna('').astype(str).str.translate(translator).str.lower().tolist()\n",
        "    hypothesis = dataframe[hypothesis_col].fillna('').astype(str).str.translate(translator).str.lower().tolist()\n",
        "\n",
        "    # Process the entire corpus to get overall metrics\n",
        "    error_measures = jiwer.process_words(ground_truth, hypothesis)\n",
        "\n",
        "    return error_measures\n",
        "\n",
        "def get_row_error_breakdown(row: pd.Series, ground_truth_col: str, hypothesis_col: str) -> str:\n",
        "    \"\"\"\n",
        "    Calculates the error breakdown for a single row of the DataFrame,\n",
        "    ignoring case and punctuation.\n",
        "\n",
        "    Args:\n",
        "        row (pd.Series): A single row of the DataFrame.\n",
        "        ground_truth_col (str): The name of the ground truth column.\n",
        "        hypothesis_col (str): The name of the hypothesis column.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string showing the substitutions (S), deletions (D),\n",
        "             and insertions (I) for the row.\n",
        "    \"\"\"\n",
        "    # Create a translation table to remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "    # Clean the text: remove punctuation and convert to lowercase\n",
        "    ground_truth = str(row[ground_truth_col]).translate(translator).lower()\n",
        "    hypothesis = str(row[hypothesis_col]).translate(translator).lower()\n",
        "\n",
        "    # Process just the single pair of sentences\n",
        "    measures = jiwer.process_words(ground_truth, hypothesis)\n",
        "\n",
        "    # Format the breakdown string\n",
        "    breakdown = f\"S: {measures.substitutions}, D: {measures.deletions}, I: {measures.insertions}\"\n",
        "    return breakdown\n",
        "\n",
        "\n",
        "# --- Main execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Set fixed file paths ---\n",
        "    # The input file must be in the same directory as the script, or provide a full path.\n",
        "    input_filename = 'FinalDataFrame.csv'\n",
        "    output_csv_filename = 'output_1.csv'\n",
        "\n",
        "    try:\n",
        "        # --- Load the specified CSV file ---\n",
        "        print(f\"Loading input file: '{input_filename}'...\")\n",
        "        df = pd.read_csv(input_filename)\n",
        "        print(\"File loaded successfully.\")\n",
        "\n",
        "        # Define the column names for clarity\n",
        "        ground_truth_column = 'Scripts'\n",
        "        hypothesis_column = 'Sherlog_STT'\n",
        "\n",
        "        # --- 1. Calculate and print overall WER ---\n",
        "        overall_wer_results = calculate_overall_wer(df, ground_truth_column, hypothesis_column)\n",
        "\n",
        "        # --- 2. Prepare the summary report for console and file ---\n",
        "        summary_lines = []\n",
        "        summary_lines.append(\"--- Overall Word Error Rate (WER) Calculation Results (Case-Insensitive, No Punctuation) ---\")\n",
        "        summary_lines.append(f\"Comparing hypothesis ('{hypothesis_column}') to ground truth ('{ground_truth_column}')\\n\")\n",
        "        summary_lines.append(f\"Overall Word Error Rate (WER): {overall_wer_results.wer:.2%}\")\n",
        "        summary_lines.append(f\"Overall Merge Error Rate (MER): {overall_wer_results.mer:.2%}\")\n",
        "        summary_lines.append(f\"Overall Word Information Lost (WIL): {overall_wer_results.wil:.2f}\\n\")\n",
        "        summary_lines.append(\"--- Overall Detailed Error Breakdown ---\")\n",
        "        summary_lines.append(f\"Hits (Correct Words):         {overall_wer_results.hits}\")\n",
        "        summary_lines.append(f\"Substitutions (S):            {overall_wer_results.substitutions}\")\n",
        "        summary_lines.append(f\"Deletions (D):                {overall_wer_results.deletions}\")\n",
        "        summary_lines.append(f\"Insertions (I):               {overall_wer_results.insertions}\\n\")\n",
        "\n",
        "        total_words_truth = overall_wer_results.hits + overall_wer_results.substitutions + overall_wer_results.deletions\n",
        "        total_words_hypothesis = overall_wer_results.hits + overall_wer_results.substitutions + overall_wer_results.insertions\n",
        "\n",
        "        summary_lines.append(\"--- Overall Corpus Statistics ---\")\n",
        "        summary_lines.append(f\"Total Words in Ground Truth:  {total_words_truth}\")\n",
        "        summary_lines.append(f\"Total Words in Hypothesis:    {total_words_hypothesis}\")\n",
        "\n",
        "        # Print the summary to the console\n",
        "        print(\"\\n\".join(summary_lines))\n",
        "\n",
        "        # --- 3. Calculate row-by-row breakdown and add to DataFrame ---\n",
        "        print(\"\\n--- Processing row-by-row error breakdown ---\")\n",
        "        df['Error_Breakdown'] = df.apply(\n",
        "            get_row_error_breakdown,\n",
        "            axis=1, # Apply function to each row\n",
        "            args=(ground_truth_column, hypothesis_column)\n",
        "        )\n",
        "        print(\"Successfully added 'Error_Breakdown' column.\")\n",
        "\n",
        "        # --- 4. Export the results to CSV and TXT files ---\n",
        "        # Export DataFrame to CSV\n",
        "        df.to_csv(output_csv_filename, index=False, encoding='utf-8')\n",
        "        print(f\"\\nResults successfully exported to '{output_csv_filename}'\")\n",
        "\n",
        "        # Generate the text file name from the CSV file name\n",
        "        base_name = os.path.splitext(output_csv_filename)[0]\n",
        "        output_txt_filename = base_name + \"_summary.txt\"\n",
        "\n",
        "        # Write the summary report to a text file\n",
        "        with open(output_txt_filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"\\n\".join(summary_lines))\n",
        "        print(f\"Overall summary successfully exported to '{output_txt_filename}'\")\n",
        "\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "    except FileNotFoundError:\n",
        "         print(f\"Error: The file '{input_filename}' was not found. Please make sure it's in the same directory as the script.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yglxVjXdNRkE",
        "outputId": "68613d8d-a13f-4a67-a575-a21bea44da08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading input file: 'FinalDataFrame.csv'...\n",
            "File loaded successfully.\n",
            "--- Overall Word Error Rate (WER) Calculation Results (Case-Insensitive, No Punctuation) ---\n",
            "Comparing hypothesis ('Sherlog_STT') to ground truth ('Scripts')\n",
            "\n",
            "Overall Word Error Rate (WER): 6.47%\n",
            "Overall Merge Error Rate (MER): 6.44%\n",
            "Overall Word Information Lost (WIL): 0.11\n",
            "\n",
            "--- Overall Detailed Error Breakdown ---\n",
            "Hits (Correct Words):         2457\n",
            "Substitutions (S):            124\n",
            "Deletions (D):                32\n",
            "Insertions (I):               13\n",
            "\n",
            "--- Overall Corpus Statistics ---\n",
            "Total Words in Ground Truth:  2613\n",
            "Total Words in Hypothesis:    2594\n",
            "\n",
            "--- Processing row-by-row error breakdown ---\n",
            "Successfully added 'Error_Breakdown' column.\n",
            "\n",
            "Results successfully exported to 'output_1.csv'\n",
            "Overall summary successfully exported to 'output_1_summary.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ComprehensiveScores"
      ],
      "metadata": {
        "id": "V7FrOlm8Perg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "import re\n",
        "\n",
        "def parse_gemini_bulk_response(response_text):\n",
        "    \"\"\"\n",
        "    Parses a multi-row markdown table from the Gemini response to extract evaluation data.\n",
        "    Returns a list of dictionaries, where each dictionary represents a row.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    lines = response_text.strip().splitlines()\n",
        "\n",
        "    # Find all lines that are part of the markdown table\n",
        "    table_lines = [line for line in lines if line.strip().startswith('|')]\n",
        "\n",
        "    # The first table line is the header, the second is the separator. Data starts from the third.\n",
        "    # This robustly skips the header and separator, preventing the header from being treated as data.\n",
        "    if len(table_lines) < 3:\n",
        "        # Not enough lines for a valid table with at least one data row\n",
        "        return []\n",
        "\n",
        "    data_lines = table_lines[2:] # Skip the | Header | and |---| lines\n",
        "\n",
        "    for line in data_lines:\n",
        "        # Use regex to robustly capture content within pipes.\n",
        "        # This handles cases where columns might be empty.\n",
        "        match = re.search(r'\\|\\s*(.*?)\\s*\\|\\s*(.*?)\\s*\\|\\s*(.*?)\\s*\\|\\s*(.*?)\\s*\\|', line)\n",
        "        if match:\n",
        "            question, answer, score, justification = [m.strip() for m in match.groups()]\n",
        "            results.append({\n",
        "                \"Question\": question,\n",
        "                \"Answer\": answer,\n",
        "                \"Score (1-10)\": score,\n",
        "                \"Justification\": justification\n",
        "            })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def evaluate_qa_pairs(input_csv_path):\n",
        "    \"\"\"\n",
        "    Reads all question-answer pairs from a CSV file, sends them to the Gemini API in a single batch,\n",
        "    and returns the structured evaluation as a list of dictionaries.\n",
        "    \"\"\"\n",
        "    # --- 1. API Configuration ---\n",
        "    # It's recommended to use environment variables for API keys for better security.\n",
        "    # For example: api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
        "    api_key = \"AIzaSyDorS4mD54chTzau2WahFrHYcATpNv5lhg\" # Please replace with your actual API key or use environment variables\n",
        "    if api_key == \"YOUR_API_KEY_HERE\":\n",
        "        print(\"Warning: Please replace 'YOUR_API_KEY_HERE' with your actual Gemini API key.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        genai.configure(api_key=api_key)\n",
        "    except Exception as e:\n",
        "        print(f\"Error configuring the Gemini API. Please check your API key. Details: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 2. Model Initialization ---\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-2.5-pro') # Updated to a recommended model\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing the model. Please check the model name and your API access. Details: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 3. Load the CSV File ---\n",
        "    try:\n",
        "        df = pd.read_csv(input_csv_path)\n",
        "        print(f\"Successfully loaded '{os.path.basename(input_csv_path)}'.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{input_csv_path}' was not found.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while reading the CSV file. Details: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- 4. Validate Columns ---\n",
        "    if \"Scripts\" not in df.columns or \"Response\" not in df.columns:\n",
        "        print(\"Error: The CSV file must contain a 'Scripts' column for questions and a 'Response' column for answers.\")\n",
        "        return None\n",
        "\n",
        "    # --- 5. Prepare all Q&A pairs for the prompt ---\n",
        "    qa_pairs_string = \"\"\n",
        "    original_qa = []\n",
        "    for index, row in df.iterrows():\n",
        "        question = str(row['Scripts']).strip()\n",
        "        answer = str(row['Response']).strip()\n",
        "\n",
        "        if question and answer and question.lower() != 'nan' and answer.lower() != 'nan':\n",
        "            qa_pairs_string += f\"Question: \\\"{question}\\\"\\nAnswer: \\\"{answer}\\\"\\n---\\n\"\n",
        "            original_qa.append({\"Question\": question, \"Answer\": answer})\n",
        "\n",
        "    if not qa_pairs_string:\n",
        "        print(\"No valid Q&A pairs found in the file.\")\n",
        "        return []\n",
        "\n",
        "    # --- 6. Construct a Single Prompt for all pairs ---\n",
        "    prompt = f\"\"\"\n",
        "    Please act as an impartial evaluator. Your task is to analyze all of the following question-and-answer pairs.\n",
        "    Provide your complete evaluation in a single, multi-row markdown table. Each row in the table must correspond to one Q&A pair.\n",
        "\n",
        "    **Q&A Pairs to Evaluate:**\n",
        "    {qa_pairs_string}\n",
        "\n",
        "    **Output Format (Strict):**\n",
        "    Create a single markdown table with exactly four columns: \"Question\", \"Answer\", \"Score (1-10)\", and \"Justification\".\n",
        "    The table must have one data row for each Q&A pair provided above. Do not include any text outside of this table.\n",
        "    | Question | Answer | Score (1-10) | Justification |\n",
        "    |---|---|---|---|\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Sending all Q&A pairs to Gemini for evaluation in a single batch...\")\n",
        "\n",
        "    try:\n",
        "        # --- 7. Generate Content with a Single API Call ---\n",
        "        response = model.generate_content(prompt)\n",
        "        print(\"Received response from Gemini. Parsing results...\")\n",
        "\n",
        "        parsed_data = parse_gemini_bulk_response(response.text)\n",
        "\n",
        "        if parsed_data:\n",
        "            print(f\"Successfully parsed {len(parsed_data)} entries from the response.\")\n",
        "            return parsed_data\n",
        "        else:\n",
        "            print(\"Could not parse the response from Gemini. The raw response was:\")\n",
        "            print(response.text)\n",
        "            # Return error placeholder for user to inspect the output file\n",
        "            return [{\"Question\": \"PARSE_ERROR\", \"Answer\": \"Could not parse Gemini's response.\", \"Score (1-10)\": \"N/A\", \"Justification\": response.text}]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"--- An error occurred with the API call: {e} ---\")\n",
        "        return [{\"Question\": \"API_ERROR\", \"Answer\": str(e), \"Score (1-10)\": \"N/A\", \"Justification\": \"The API call failed.\"}]\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- 1. Set fixed input and output file names ---\n",
        "    input_file = 'FinalDataFrame.csv'\n",
        "    output_file = 'output_2.csv'\n",
        "\n",
        "    print(f\"Input file set to: {input_file}\")\n",
        "    print(f\"Output file will be saved as: {output_file}\")\n",
        "\n",
        "    # --- 2. Run the evaluation ---\n",
        "    results = evaluate_qa_pairs(input_file)\n",
        "\n",
        "    # --- 3. Save results to new CSV ---\n",
        "    if results:\n",
        "        print(f\"\\nEvaluation complete. Processed {len(results)} entries.\")\n",
        "        output_df = pd.DataFrame(results)\n",
        "\n",
        "        try:\n",
        "            output_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
        "            print(f\"Results successfully saved to '{output_file}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to save the results to CSV. Error: {e}\")\n",
        "    else:\n",
        "        print(\"Evaluation did not produce any results.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "ZOu8mbKqP3En",
        "outputId": "cb0ef7eb-db8e-4162-9e42-72a7f20ad970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input file set to: FinalDataFrame.csv\n",
            "Output file will be saved as: output_2.csv\n",
            "Successfully loaded 'FinalDataFrame.csv'.\n",
            "Sending all Q&A pairs to Gemini for evaluation in a single batch...\n",
            "Received response from Gemini. Parsing results...\n",
            "Successfully parsed 294 entries from the response.\n",
            "\n",
            "Evaluation complete. Processed 294 entries.\n",
            "Results successfully saved to 'output_2.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ContentSwapScores"
      ],
      "metadata": {
        "id": "HQCYuPcePGN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "# import tkinter as tk  # No longer needed for file dialogs\n",
        "# from tkinter import filedialog, messagebox # No longer needed for file dialogs\n",
        "\n",
        "# --- Configuration ---\n",
        "LOW_IMPACT_SWAPS = {\n",
        "    frozenset({'at', 'in'}),\n",
        "    frozenset({'on', 'in'}),\n",
        "    frozenset({'a', 'the'}),\n",
        "    frozenset({'a', 'of'}),\n",
        "}\n",
        "NUMBER_WORDS = {'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', 'ten': '10'}\n",
        "\n",
        "# NEW: Add a list of common, low-impact \"stop words\".\n",
        "# Deleting one of these is a minor error. Deleting anything else is critical.\n",
        "STOP_WORDS = {\n",
        "    'a', 'an', 'the', 'is', 'are', 'was', 'were', 'in', 'on', 'at', 'of',\n",
        "    'to', 'for', 'by', 'with', 'and', 'or', 'but', 'if', 'be'\n",
        "}\n",
        "\n",
        "# --- Normalization and Reconciliation Functions ---\n",
        "def normalize_text(text: str) -> str:\n",
        "    # ... (This function is unchanged from your provided version) ...\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = text.lower()\n",
        "    text = text.replace('$', 'dollar ')\n",
        "    text = re.sub(r'(\\w+)-dollar', r'dollar \\1', text)\n",
        "    text = re.sub(r'\\b(are)\\b', \"'re\", text)\n",
        "    text = re.sub(r\"[’‘´]\", \"'\", text)\n",
        "    text = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', text)\n",
        "    text = ' '.join([NUMBER_WORDS.get(word, word) for word in text.split()])\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    text = text.strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "def reconcile_spacing_errors(words1: list, words2: list) -> bool:\n",
        "    # ... (This function is unchanged) ...\n",
        "    if len(words1) > len(words2): longer_list, shorter_list = words1, words2\n",
        "    elif len(words2) > len(words1): longer_list, shorter_list = words2, words1\n",
        "    else: return False\n",
        "    shorter_word_set = set(shorter_list)\n",
        "    for i in range(len(longer_list) - 1):\n",
        "        combined_word = longer_list[i] + longer_list[i+1]\n",
        "        if combined_word in shorter_word_set:\n",
        "            repaired_list = longer_list[:i] + [combined_word] + longer_list[i+2:]\n",
        "            if sorted(repaired_list) == sorted(shorter_list):\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# --- Scoring Functions ---\n",
        "# ... (These functions are unchanged) ...\n",
        "def get_substitution_cost(c1: str, c2: str) -> float:\n",
        "    vowels = \"aeiou\"\n",
        "    phonetically_similar_pairs = [('t', 'd'), ('p', 'b'), ('k', 'g'), ('s', 'z'), ('f', 'v')]\n",
        "    if c1 == c2: return 0.0\n",
        "    if c1 in vowels and c2 in vowels: return 0.4\n",
        "    for p1, p2 in phonetically_similar_pairs:\n",
        "        if (c1 == p1 and c2 == p2) or (c1 == p2 and c2 == p1): return 0.5\n",
        "    return 1.0\n",
        "\n",
        "def weighted_levenshtein(s1: str, s2: str) -> float:\n",
        "    if len(s1) < len(s2): return weighted_levenshtein(s2, s1)\n",
        "    if len(s2) == 0: return float(len(s1))\n",
        "    previous_row = list(range(len(s2) + 1))\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions, deletions = previous_row[j + 1] + 1, current_row[j] + 1\n",
        "            substitutions = previous_row[j] + get_substitution_cost(c1, c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    return float(previous_row[-1])\n",
        "\n",
        "def calculate_jaccard_similarity(set1: set, set2: set) -> float:\n",
        "    if not set1 and not set2: return 1.0\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union if union != 0 else 0.0\n",
        "\n",
        "# --- MODIFIED: Intent Classification Function ---\n",
        "def classify_intent(norm_source: str, norm_asr: str, semantic_score: float) -> str:\n",
        "    source_words, asr_words = set(norm_source.split()), set(norm_asr.split())\n",
        "    question_words = {'what', 'why', 'who', 'how', 'which', 'where', 'when'}\n",
        "    deleted_words, added_words = source_words - asr_words, asr_words - source_words\n",
        "\n",
        "    # --- Swap Rules (Highest Priority) ---\n",
        "    if len(deleted_words) == 1 and len(added_words) == 1:\n",
        "        swapped_pair = frozenset(deleted_words.union(added_words))\n",
        "        if swapped_pair not in LOW_IMPACT_SWAPS:\n",
        "            return \"Swaps Intent\"\n",
        "    source_qword, asr_qword = question_words.intersection(source_words), question_words.intersection(asr_words)\n",
        "    if source_qword and asr_qword and (source_qword != asr_qword):\n",
        "        return \"Swaps Intent\"\n",
        "\n",
        "    # --- NEW: Critical Deletion Rule ---\n",
        "    # If exactly one important (non-stop word) was deleted, the intent is obscured.\n",
        "    if len(deleted_words) == 1 and len(added_words) == 0:\n",
        "        deleted_word = list(deleted_words)[0]\n",
        "        if deleted_word not in STOP_WORDS:\n",
        "            return \"Obscures Intent\"\n",
        "    # --- END NEW RULE ---\n",
        "\n",
        "    # --- Other Obscures Intent Rules ---\n",
        "    if len(source_words) <= 5 and semantic_score != 1.0:\n",
        "        return \"Obscures Intent\"\n",
        "    if source_qword and not asr_qword:\n",
        "        return \"Obscures Intent\"\n",
        "    if semantic_score < 0.7:\n",
        "        return \"Obscures Intent\"\n",
        "\n",
        "    return \"Preserves Intent\"\n",
        "\n",
        "def get_risk_score(intent_category: str) -> int:\n",
        "    return {\"Preserves Intent\": 0, \"Obscures Intent\": 1, \"Swaps Intent\": 5}.get(intent_category, -1)\n",
        "\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    # MODIFIED: Removed the file dialog for a fixed input file name\n",
        "    # 1. Fixed the input file name\n",
        "    input_csv_path = 'FinalDataFrame.csv'\n",
        "\n",
        "    source_col = 'Source (Speaker1)'\n",
        "    asr_col = 'ASR on Device under test'\n",
        "    try:\n",
        "        df = pd.read_csv(input_csv_path, sep=',')\n",
        "        results = []\n",
        "        print(f\"Starting analysis on '{input_csv_path}'...\")\n",
        "        for index, row in df.iterrows():\n",
        "            source_signal, asr_hypothesis = row.get(source_col, \"\"), row.get(asr_col, \"\")\n",
        "            norm_source, norm_asr = normalize_text(source_signal), normalize_text(asr_hypothesis)\n",
        "            phonetic_score, semantic_score, intent_category, risk_score = 0.0, 1.0, \"Preserves Intent\", 0\n",
        "            if norm_source != norm_asr:\n",
        "                was_reconciled = reconcile_spacing_errors(norm_source.split(), norm_asr.split())\n",
        "                if was_reconciled:\n",
        "                    phonetic_score, semantic_score, intent_category, risk_score = 0.5, 1.0, \"Preserves Intent\", 0\n",
        "                else:\n",
        "                    phonetic_score = weighted_levenshtein(norm_source, norm_asr)\n",
        "                    source_set, asr_set = set(norm_source.split()), set(asr_hypothesis.split())\n",
        "                    semantic_score = calculate_jaccard_similarity(source_set, asr_set)\n",
        "                    intent_category = classify_intent(norm_source, norm_asr, semantic_score)\n",
        "                    risk_score = get_risk_score(intent_category)\n",
        "            results.append([phonetic_score, semantic_score, intent_category, risk_score])\n",
        "\n",
        "        new_columns_df = pd.DataFrame(results, columns=['Phonetic_Distance_Score', 'Semantic_Similarity_Score', 'Impact_on_Intent_Category', 'Intent_Impact_Risk_Score'])\n",
        "        cols_to_drop = new_columns_df.columns.tolist()\n",
        "        df_clean = df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors='ignore')\n",
        "        final_df = pd.concat([df_clean.reset_index(drop=True), new_columns_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "        # MODIFIED: Removed file dialog for a fixed output file name\n",
        "        # 2. Fixed the output file name\n",
        "        output_csv_path = 'output_3.csv'\n",
        "\n",
        "        final_df.to_csv(output_csv_path, index=False, sep=',')\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"✅ Analysis complete!\")\n",
        "        print(f\"Results have been saved to '{output_csv_path}'\")\n",
        "        print(\"-\" * 50)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ ERROR: Input file not found.\")\n",
        "        print(f\"Please make sure '{input_csv_path}' is in the same directory as the script.\")\n",
        "    except KeyError as e:\n",
        "        print(f\"❌ ERROR: A required column was not found in the CSV. Details: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDshZrTpPizl",
        "outputId": "871c7358-38a2-4915-d0f9-43922b414fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting analysis on 'FinalDataFrame.csv'...\n",
            "--------------------------------------------------\n",
            "✅ Analysis complete!\n",
            "Results have been saved to 'output_3.csv'\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conbine All CSV"
      ],
      "metadata": {
        "id": "xPFy05e1SDWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def combine_and_deduplicate_csvs(output_file):\n",
        "    \"\"\"\n",
        "    Combines the specific CSV files 'output_1.csv', 'output_2.csv', 'output_3.csv',\n",
        "    removes duplicate columns, and saves the result to a new CSV file.\n",
        "\n",
        "    Args:\n",
        "        output_file (str): The path for the output CSV file.\n",
        "    \"\"\"\n",
        "    # List of the specific input CSV files to combine\n",
        "    file_list = ['output_1.csv', 'output_2.csv', 'output_3.csv']\n",
        "\n",
        "    # Check if input files exist\n",
        "    for f in file_list:\n",
        "        if not os.path.exists(f):\n",
        "            print(f\"Error: Input file not found at '{f}'\")\n",
        "            return\n",
        "\n",
        "    try:\n",
        "        # Read each CSV file into a pandas DataFrame and store them in a list\n",
        "        df_list = [pd.read_csv(file) for file in file_list]\n",
        "\n",
        "        # Concatenate all DataFrames in the list into a single DataFrame horizontally\n",
        "        combined_df = pd.concat(df_list, axis=1)\n",
        "        print(f\"Total columns before deduplication: {len(combined_df.columns)}\")\n",
        "\n",
        "        # Remove duplicate columns by name, keeping the first occurrence\n",
        "        deduplicated_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n",
        "        print(f\"Total columns after deduplication: {len(deduplicated_df.columns)}\")\n",
        "\n",
        "        # Save the deduplicated DataFrame to a new CSV file\n",
        "        # index=False prevents pandas from writing row indices to the file\n",
        "        deduplicated_df.to_csv(output_file, index=False)\n",
        "        print(f\"Successfully combined and deduplicated files into '{output_file}'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # The script now uses a hardcoded list of files inside the function.\n",
        "    # We just need to define the output filename.\n",
        "    output_filename = 'combined_output.csv'\n",
        "\n",
        "    # Run the function\n",
        "    combine_and_deduplicate_csvs(output_filename)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSZyWZxXSA1c",
        "outputId": "357f5e1f-da8c-4a6e-d290-5c5d4a032e2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total columns before deduplication: 27\n",
            "Total columns after deduplication: 18\n",
            "Successfully combined and deduplicated files into 'combined_output.csv'\n"
          ]
        }
      ]
    }
  ]
}